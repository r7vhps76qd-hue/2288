"""
AI-–∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ Telegram –∞—Ä—Ö–∏–≤–æ–≤
"""
import os
import json
import re
from datetime import datetime
from collections import Counter
import zipfile
import tempfile
import shutil

class AIAnalyzer:
    def __init__(self, storage_path="./secure_storage"):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è AI –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
        
        Args:
            storage_path: –ü—É—Ç—å –∫ —Ö—Ä–∞–Ω–∏–ª–∏—â—É –¥–∞–Ω–Ω—ã—Ö
        """
        self.storage_path = storage_path
        self.decrypted_storage = f"{storage_path}/decrypted"
        self.ai_results_path = f"{storage_path}/ai_results"
        
        # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫–∏
        os.makedirs(self.ai_results_path, exist_ok=True)
        os.makedirs(f"{self.ai_results_path}/reports", exist_ok=True)
        os.makedirs(f"{self.ai_results_path}/stats", exist_ok=True)
        
        print("ü§ñ AI-–∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
    
    def analyze_telegram_archive(self, archive_path):
        """
        –ê–Ω–∞–ª–∏–∑ Telegram –∞—Ä—Ö–∏–≤–∞
        
        Args:
            archive_path: –ü—É—Ç—å –∫ –∞—Ä—Ö–∏–≤—É .zip
        
        Returns:
            dict: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞
        """
        print(f"üîç –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é –∞—Ä—Ö–∏–≤: {os.path.basename(archive_path)}")
        
        results = {
            "archive_name": os.path.basename(archive_path),
            "analysis_date": datetime.now().isoformat(),
            "basic_stats": {},
            "sentiment_analysis": {},
            "content_analysis": {},
            "user_analysis": {},
            "anomalies": [],
            "summary": ""
        }
        
        try:
            # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –ø–∞–ø–∫—É –¥–ª—è —Ä–∞—Å–ø–∞–∫–æ–≤–∫–∏
            temp_dir = tempfile.mkdtemp()
            
            # –†–∞—Å–ø–∞–∫–æ–≤—ã–≤–∞–µ–º –∞—Ä—Ö–∏–≤
            with zipfile.ZipFile(archive_path, 'r') as zip_ref:
                zip_ref.extractall(temp_dir)
            
            # –ò—â–µ–º —Ñ–∞–π–ª—ã –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
            metadata_files = []
            for root, dirs, files in os.walk(temp_dir):
                for file in files:
                    if file == 'metadata.json' or file.endswith('.json'):
                        metadata_files.append(os.path.join(root, file))
            
            if not metadata_files:
                results["summary"] = "‚ö†Ô∏è –í –∞—Ä—Ö–∏–≤–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ"
                return results
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–π —Ñ–∞–π–ª –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
            all_messages = []
            all_users = set()
            
            for metadata_file in metadata_files:
                try:
                    with open(metadata_file, 'r', encoding='utf-8') as f:
                        metadata = json.load(f)
                    
                    # –ë–∞–∑–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑
                    if 'messages' in metadata:
                        messages = metadata['messages']
                        all_messages.extend(messages)
                        
                        # –°–æ–±–∏—Ä–∞–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
                        for msg in messages:
                            if 'sender_id' in msg:
                                all_users.add(str(msg['sender_id']))
                
                except Exception as e:
                    print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è {metadata_file}: {e}")
            
            if not all_messages:
                results["summary"] = "üì≠ –í –∞—Ä—Ö–∏–≤–µ –Ω–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞"
                return results
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º –∞–Ω–∞–ª–∏–∑
            results["basic_stats"] = self._analyze_basic_stats(all_messages, all_users)
            results["sentiment_analysis"] = self._analyze_sentiment(all_messages)
            results["content_analysis"] = self._analyze_content(all_messages)
            results["user_analysis"] = self._analyze_users(all_messages)
            results["anomalies"] = self._detect_anomalies(all_messages)
            results["summary"] = self._generate_summary(results)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            self._save_results(results, archive_path)
            
            # –û—á–∏—â–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –ø–∞–ø–∫—É
            shutil.rmtree(temp_dir)
            
            print(f"‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω: {len(all_messages)} —Å–æ–æ–±—â–µ–Ω–∏–π, {len(all_users)} –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π")
            return results
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –∞—Ä—Ö–∏–≤–∞: {e}")
            results["summary"] = f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {str(e)}"
            return results
    
    def _analyze_basic_stats(self, messages, users):
        """–ë–∞–∑–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞"""
        stats = {
            "total_messages": len(messages),
            "unique_users": len(users),
            "time_period": {},
            "media_count": 0,
            "avg_message_length": 0
        }
        
        # –í—Ä–µ–º–µ–Ω–Ω–æ–π –ø–µ—Ä–∏–æ–¥
        dates = []
        total_length = 0
        
        for msg in messages:
            # –î–∞—Ç–∞ —Å–æ–æ–±—â–µ–Ω–∏—è
            if 'date' in msg and msg['date']:
                try:
                    date_str = msg['date'].split('T')[0] if 'T' in msg['date'] else msg['date']
                    dates.append(date_str)
                except:
                    pass
            
            # –î–ª–∏–Ω–∞ —Å–æ–æ–±—â–µ–Ω–∏—è
            if 'text' in msg and msg['text']:
                total_length += len(str(msg['text']))
            
            # –ú–µ–¥–∏–∞
            if 'media_type' in msg and msg['media_type']:
                stats["media_count"] += 1
        
        if dates:
            stats["time_period"] = {
                "first_date": min(dates),
                "last_date": max(dates),
                "days_span": (datetime.fromisoformat(max(dates)) - datetime.fromisoformat(min(dates))).days
            }
        
        if messages:
            stats["avg_message_length"] = total_length / len(messages)
        
        return stats
    
    def _analyze_sentiment(self, messages):
        """–ê–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ (—É–ø—Ä–æ—â–µ–Ω–Ω—ã–π)"""
        sentiment = {
            "positive_words": 0,
            "negative_words": 0,
            "neutral_words": 0,
            "sentiment_score": 0,
            "dominant_emotion": "neutral"
        }
        
        # –°–ø–∏—Å–∫–∏ —Å–ª–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        positive_words = {
            '—Ö–æ—Ä–æ—à–æ', '–æ—Ç–ª–∏—á–Ω–æ', '–ø—Ä–µ–∫—Ä–∞—Å–Ω–æ', '–∑–∞–º–µ—á–∞—Ç–µ–ª—å–Ω–æ', '—Å—É–ø–µ—Ä', '–∫–ª–∞—Å—Å', '–æ—Ç–ª–∏—á–Ω—ã–π',
            '—Ö–æ—Ä–æ—à–∏–π', '–ø—Ä–µ–∫—Ä–∞—Å–Ω—ã–π', '–∑–∞–º–µ—á–∞—Ç–µ–ª—å–Ω—ã–π', '–≤–µ–ª–∏–∫–æ–ª–µ–ø–Ω–æ', '–ø—Ä–µ–≤–æ—Å—Ö–æ–¥–Ω–æ',
            '—Å–ø–∞—Å–∏–±–æ', '–±–ª–∞–≥–æ–¥–∞—Ä—é', '—Ä–∞–¥', '–¥–æ–≤–æ–ª–µ–Ω', '—Å—á–∞—Å—Ç–ª–∏–≤', '—É—Å–ø–µ—Ö', '–ø–æ–±–µ–¥–∞',
            '–ª—é–±–æ–≤—å', '–Ω—Ä–∞–≤–∏—Ç—Å—è', '–≤–æ—Å—Ö–∏—Ç–∏—Ç–µ–ª—å–Ω–æ', '–ø–æ—Ç—Ä—è—Å–∞—é—â–µ', '–∑–¥–æ—Ä–æ–≤–æ'
        }
        
        negative_words = {
            '–ø–ª–æ—Ö–æ', '—É–∂–∞—Å–Ω–æ', '–æ—Ç–≤—Ä–∞—Ç–∏—Ç–µ–ª—å–Ω–æ', '–∫–æ—à–º–∞—Ä', '–ø—Ä–æ–±–ª–µ–º–∞', '–æ—à–∏–±–∫–∞',
            '–Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ', '–Ω–µ–ª—å–∑—è', '–∑–∞–ø—Ä–µ—â–µ–Ω–æ', '–æ–ø–∞—Å–Ω–æ', '—Å—Ç—Ä–∞—à–Ω–æ', '–≥—Ä—É—Å—Ç–Ω–æ',
            '–ø–µ—á–∞–ª—å–Ω–æ', '—Ä–∞–∑–æ—á–∞—Ä–æ–≤–∞–Ω', '–∑–ª–æ–π', '—Å–µ—Ä–¥–∏—Ç—ã–π', '–Ω–µ–Ω–∞–≤–∏–∂—É', '–Ω–µ –ª—é–±–ª—é',
            '–ø—Ä–æ–∏–≥—Ä—ã—à', '–ø–æ—Ä–∞–∂–µ–Ω–∏–µ', '–ø—Ä–æ–≤–∞–ª', '–∫–∞—Ç–∞—Å—Ç—Ä–æ—Ñ–∞', '–±–µ–¥–∞'
        }
        
        total_words = 0
        
        for msg in messages:
            if 'text' in msg and msg['text']:
                text = str(msg['text']).lower()
                words = re.findall(r'\b[–∞-—èa-z]+\b', text)
                
                for word in words:
                    total_words += 1
                    if word in positive_words:
                        sentiment["positive_words"] += 1
                    elif word in negative_words:
                        sentiment["negative_words"] += 1
                    else:
                        sentiment["neutral_words"] += 1
        
        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º score
        if total_words > 0:
            positive_ratio = sentiment["positive_words"] / total_words
            negative_ratio = sentiment["negative_words"] / total_words
            sentiment["sentiment_score"] = positive_ratio - negative_ratio
            
            if sentiment["sentiment_score"] > 0.1:
                sentiment["dominant_emotion"] = "positive"
            elif sentiment["sentiment_score"] < -0.1:
                sentiment["dominant_emotion"] = "negative"
            else:
                sentiment["dominant_emotion"] = "neutral"
        
        return sentiment
    
    def _analyze_content(self, messages):
        """–ê–Ω–∞–ª–∏–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        content = {
            "common_words": [],
            "message_frequency": {},
            "urls_count": 0,
            "hashtags_count": 0,
            "mentions_count": 0
        }
        
        # –°—á–µ—Ç—á–∏–∫ —Å–ª–æ–≤
        word_counter = Counter()
        stop_words = {'–∏', '–≤', '–Ω–µ', '–Ω–∞', '—á—Ç–æ', '—ç—Ç–æ', '–∫–∞–∫', '–Ω–æ', '–∞', '–∏–ª–∏', '—É', '–∑–∞', '–∫', '–¥–æ', '–ø–æ', '–∏–∑', '–æ—Ç', '–∂–µ', '–±—ã', '–¥–ª—è', '—Ç–æ', '–≤—ã', '–æ–Ω', '–æ–Ω–∞', '–æ–Ω–∏', '–º—ã', '–≤–∞—Å', '–≤–∞—à', '–∏—Ö', '—Ç–µ', '—Ç–∞', '—Ç–æ—Ç', '—ç—Ç–æ—Ç', '—Ç–∞–∫–æ–π', '—Ç–∞–∫–∏–µ', '—Å–≤–æ–π'}
        
        for msg in messages:
            if 'text' in msg and msg['text']:
                text = str(msg['text']).lower()
                
                # –°—á–∏—Ç–∞–µ–º —Å–ª–æ–≤–∞
                words = re.findall(r'\b[–∞-—èa-z]{3,}\b', text)
                for word in words:
                    if word not in stop_words:
                        word_counter[word] += 1
                
                # –°—á–∏—Ç–∞–µ–º URL
                content["urls_count"] += len(re.findall(r'https?://\S+', text))
                
                # –°—á–∏—Ç–∞–µ–º —Ö—ç—à—Ç–µ–≥–∏
                content["hashtags_count"] += len(re.findall(r'#\w+', text))
                
                # –°—á–∏—Ç–∞–µ–º —É–ø–æ–º–∏–Ω–∞–Ω–∏—è
                content["mentions_count"] += len(re.findall(r'@\w+', text))
        
        # –°–∞–º—ã–µ —á–∞—Å—Ç—ã–µ —Å–ª–æ–≤–∞
        content["common_words"] = word_counter.most_common(20)
        
        return content
    
    def _analyze_users(self, messages):
        """–ê–Ω–∞–ª–∏–∑ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π"""
        user_analysis = {
            "top_posters": [],
            "user_activity": {},
            "avg_messages_per_user": 0
        }
        
        # –°—á–∏—Ç–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è –ø–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º
        user_counter = Counter()
        
        for msg in messages:
            if 'sender_id' in msg:
                user_counter[str(msg['sender_id'])] += 1
        
        # –¢–æ–ø –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
        user_analysis["top_posters"] = user_counter.most_common(10)
        
        # –ê–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø–æ –≤—Ä–µ–º–µ–Ω–∏
        time_counter = Counter()
        for msg in messages:
            if 'date' in msg and msg['date']:
                try:
                    hour = datetime.fromisoformat(msg['date'].replace('Z', '+00:00')).hour
                    time_counter[hour] += 1
                except:
                    pass
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Å–ª–æ–≤–∞—Ä—å
        user_analysis["user_activity"] = dict(time_counter)
        
        # –°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–æ–±—â–µ–Ω–∏–π
        if user_counter:
            user_analysis["avg_messages_per_user"] = len(messages) / len(user_counter)
        
        return user_analysis
    
    def _detect_anomalies(self, messages):
        """–û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∞–Ω–æ–º–∞–ª–∏–π"""
        anomalies = []
        
        if not messages:
            return anomalies
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ —Å–ø–∞–º (–º–Ω–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏–π –æ—Ç –æ–¥–Ω–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∑–∞ –∫–æ—Ä–æ—Ç–∫–æ–µ –≤—Ä–µ–º—è)
        user_messages = {}
        for msg in messages:
            if 'sender_id' in msg and 'date' in msg:
                user_id = msg['sender_id']
                if user_id not in user_messages:
                    user_messages[user_id] = []
                user_messages[user_id].append(msg['date'])
        
        for user_id, dates in user_messages.items():
            if len(dates) > 50:  # –ú–Ω–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏–π
                try:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω–æ–π –∏–Ω—Ç–µ—Ä–≤–∞–ª
                    sorted_dates = sorted([datetime.fromisoformat(d.replace('Z', '+00:00')) for d in dates])
                    time_span = (sorted_dates[-1] - sorted_dates[0]).total_seconds()
                    
                    if time_span < 3600 and len(dates) > 20:  # 20+ —Å–æ–æ–±—â–µ–Ω–∏–π –∑–∞ —á–∞—Å
                        anomalies.append({
                            "type": "possible_spam",
                            "user_id": user_id,
                            "messages_count": len(dates),
                            "time_span_seconds": time_span
                        })
                except:
                    pass
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –æ—á–µ–Ω—å –¥–ª–∏–Ω–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è
        for msg in messages:
            if 'text' in msg and msg['text']:
                text_len = len(str(msg['text']))
                if text_len > 1000:
                    anomalies.append({
                        "type": "very_long_message",
                        "message_id": msg.get('id', 'unknown'),
                        "length": text_len
                    })
        
        return anomalies
    
    def _generate_summary(self, analysis_results):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —Ä–µ–∑—é–º–µ"""
        stats = analysis_results["basic_stats"]
        sentiment = analysis_results["sentiment_analysis"]
        content = analysis_results["content_analysis"]
        
        summary_lines = []
        
        summary_lines.append(f"üìä –û–ë–©–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
        summary_lines.append(f"‚Ä¢ –°–æ–æ–±—â–µ–Ω–∏–π: {stats['total_messages']}")
        summary_lines.append(f"‚Ä¢ –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π: {stats['unique_users']}")
        
        if 'time_period' in stats and stats['time_period']:
            tp = stats['time_period']
            summary_lines.append(f"‚Ä¢ –ü–µ—Ä–∏–æ–¥: {tp.get('first_date', '?')} - {tp.get('last_date', '?')}")
            summary_lines.append(f"‚Ä¢ –î–Ω–µ–π –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏: {tp.get('days_span', '?')}")
        
        summary_lines.append(f"‚Ä¢ –ú–µ–¥–∏–∞—Ñ–∞–π–ª–æ–≤: {stats.get('media_count', 0)}")
        summary_lines.append(f"‚Ä¢ –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —Å–æ–æ–±—â–µ–Ω–∏—è: {stats.get('avg_message_length', 0):.0f} —Å–∏–º–≤.")
        
        summary_lines.append(f"\nüé≠ –¢–û–ù–ê–õ–¨–ù–û–°–¢–¨:")
        summary_lines.append(f"‚Ä¢ –ü—Ä–µ–æ–±–ª–∞–¥–∞—é—â–∞—è —ç–º–æ—Ü–∏—è: {sentiment.get('dominant_emotion', 'neutral').upper()}")
        summary_lines.append(f"‚Ä¢ –û—Ü–µ–Ω–∫–∞: {sentiment.get('sentiment_score', 0):.2f}")
        summary_lines.append(f"‚Ä¢ –ü–æ–∑–∏—Ç–∏–≤–Ω—ã—Ö —Å–ª–æ–≤: {sentiment.get('positive_words', 0)}")
        summary_lines.append(f"‚Ä¢ –ù–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö —Å–ª–æ–≤: {sentiment.get('negative_words', 0)}")
        
        summary_lines.append(f"\nüîç –ö–û–ù–¢–ï–ù–¢:")
        summary_lines.append(f"‚Ä¢ URL: {content.get('urls_count', 0)}")
        summary_lines.append(f"‚Ä¢ –•—ç—à—Ç–µ–≥–æ–≤: {content.get('hashtags_count', 0)}")
        summary_lines.append(f"‚Ä¢ –£–ø–æ–º–∏–Ω–∞–Ω–∏–π: {content.get('mentions_count', 0)}")
        
        if content.get('common_words'):
            top_words = ", ".join([f"{word}({count})" for word, count in content['common_words'][:5]])
            summary_lines.append(f"‚Ä¢ –ß–∞—Å—Ç—ã–µ —Å–ª–æ–≤–∞: {top_words}")
        
        if analysis_results.get('anomalies'):
            summary_lines.append(f"\n‚ö†Ô∏è  –ê–ù–û–ú–ê–õ–ò–ò:")
            for anomaly in analysis_results['anomalies'][:3]:
                summary_lines.append(f"‚Ä¢ {anomaly.get('type', 'unknown')}")
        
        return "\n".join(summary_lines)
    
    def _save_results(self, results, archive_path):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞"""
        archive_name = os.path.basename(archive_path).replace('.zip', '').replace('.enc', '')
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # JSON —Å –ø–æ–ª–Ω—ã–º–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
        json_file = f"{self.ai_results_path}/stats/{archive_name}_{timestamp}.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        # –¢–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç—á–µ—Ç
        report_file = f"{self.ai_results_path}/reports/{archive_name}_{timestamp}_report.txt"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(f"üìä AI –ê–ù–ê–õ–ò–ó –¢–ï–õ–ï–ì–†–ê–ú –ê–†–•–ò–í–ê\n")
            f.write(f"=" * 50 + "\n\n")
            f.write(f"üìÅ –ê—Ä—Ö–∏–≤: {results['archive_name']}\n")
            f.write(f"üìÖ –î–∞—Ç–∞ –∞–Ω–∞–ª–∏–∑–∞: {results['analysis_date']}\n")
            f.write(f"=" * 50 + "\n\n")
            f.write(results['summary'])
        
        print(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã:")
        print(f"   üìä JSON: {json_file}")
        print(f"   üìù –û—Ç—á–µ—Ç: {report_file}")
    
    def analyze_all_archives(self):
        """–ê–Ω–∞–ª–∏–∑ –≤—Å–µ—Ö –∞—Ä—Ö–∏–≤–æ–≤ –≤ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ"""
        archives_path = self.decrypted_storage
        
        if not os.path.exists(archives_path):
            print(f"‚ùå –ü–∞–ø–∫–∞ —Å –∞—Ä—Ö–∏–≤–∞–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {archives_path}")
            return []
        
        # –ò—â–µ–º .zip —Ñ–∞–π–ª—ã
        archives = []
        for file in os.listdir(archives_path):
            if file.endswith('.zip'):
                archives.append(os.path.join(archives_path, file))
        
        print(f"üìÅ –ù–∞–π–¥–µ–Ω–æ –∞—Ä—Ö–∏–≤–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞: {len(archives)}")
        
        results = []
        for archive in archives:
            result = self.analyze_telegram_archive(archive)
            results.append(result)
        
        # –°–æ–∑–¥–∞–µ–º –æ–±—â–∏–π –æ—Ç—á–µ—Ç
        if results:
            self._create_global_report(results)
        
        return results
    
    def _create_global_report(self, all_results):
        """–°–æ–∑–¥–∞–Ω–∏–µ –æ–±—â–µ–≥–æ –æ—Ç—á–µ—Ç–∞ –ø–æ –≤—Å–µ–º –∞—Ä—Ö–∏–≤–∞–º"""
        if not all_results:
            return
        
        total_messages = sum(r['basic_stats'].get('total_messages', 0) for r in all_results)
        total_users = sum(r['basic_stats'].get('unique_users', 0) for r in all_results)
        
        # –ê–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
        sentiment_scores = [r['sentiment_analysis'].get('sentiment_score', 0) for r in all_results]
        avg_sentiment = sum(sentiment_scores) / len(sentiment_scores) if sentiment_scores else 0
        
        report = f"""
üåê –û–ë–©–ò–ô –û–¢–ß–ï–¢ –ü–û –ê–†–•–ò–í–ê–ú
{"=" * 50}

üìä –û–ë–©–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:
‚Ä¢ –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –∞—Ä—Ö–∏–≤–æ–≤: {len(all_results)}
‚Ä¢ –í—Å–µ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏–π: {total_messages}
‚Ä¢ –í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π: {total_users}

üé≠ –°–†–ï–î–ù–Ø–Ø –¢–û–ù–ê–õ–¨–ù–û–°–¢–¨:
‚Ä¢ –û—Ü–µ–Ω–∫–∞: {avg_sentiment:.2f}
‚Ä¢ –û–±—â–∏–π –Ω–∞—Å—Ç—Ä–æ–π: {'–ü–û–ó–ò–¢–ò–í–ù–´–ô' if avg_sentiment > 0.1 else '–ù–ï–ì–ê–¢–ò–í–ù–´–ô' if avg_sentiment < -0.1 else '–ù–ï–ô–¢–†–ê–õ–¨–ù–´–ô'}

üìà –¢–û–ü –ê–†–•–ò–í–û–í –ü–û –ê–ö–¢–ò–í–ù–û–°–¢–ò:
"""
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —Å–æ–æ–±—â–µ–Ω–∏–π
        sorted_results = sorted(all_results, key=lambda x: x['basic_stats'].get('total_messages', 0), reverse=True)
        
        for i, result in enumerate(sorted_results[:5], 1):
            stats = result['basic_stats']
            report += f"{i}. {result['archive_name']}: {stats.get('total_messages', 0)} —Å–æ–æ–±—â–µ–Ω–∏–π, {stats.get('unique_users', 0)} –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π\n"
        
        report += f"\n‚ö†Ô∏è  –í–°–ï–ì–û –ê–ù–û–ú–ê–õ–ò–ô: {sum(len(r['anomalies']) for r in all_results)}"
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—â–∏–π –æ—Ç—á–µ—Ç
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = f"{self.ai_results_path}/reports/GLOBAL_REPORT_{timestamp}.txt"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
        
        print(f"üåê –û–±—â–∏–π –æ—Ç—á–µ—Ç —Å–æ–∑–¥–∞–Ω: {report_file}")

# –£—Ç–∏–ª–∏—Ç—ã –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –∞—Ä—Ö–∏–≤–∞–º–∏
class ArchiveManager:
    def __init__(self, storage_path="./secure_storage"):
        self.storage_path = storage_path
        self.decrypted_storage = f"{storage_path}/decrypted"
    
    def list_archives(self):
        """–°–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –∞—Ä—Ö–∏–≤–æ–≤"""
        archives = []
        
        if os.path.exists(self.decrypted_storage):
            for file in os.listdir(self.decrypted_storage):
                if file.endswith('.zip'):
                    filepath = os.path.join(self.decrypted_storage, file)
                    size = os.path.getsize(filepath) // 1024  # KB
                    archives.append({
                        'name': file,
                        'path': filepath,
                        'size_kb': size,
                        'modified': datetime.fromtimestamp(os.path.getmtime(filepath)).strftime('%Y-%m-%d %H:%M')
                    })
        
        return sorted(archives, key=lambda x: x['modified'], reverse=True)
    
    def cleanup_old_archives(self, days_old=30):
        """–û—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö –∞—Ä—Ö–∏–≤–æ–≤"""
        cutoff_date = datetime.now().timestamp() - (days_old * 24 * 3600)
        cleaned = 0
        
        for archive in self.list_archives():
            if os.path.getmtime(archive['path']) < cutoff_date:
                try:
                    os.remove(archive['path'])
                    cleaned += 1
                    print(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω —Å—Ç–∞—Ä—ã–π –∞—Ä—Ö–∏–≤: {archive['name']}")
                except Exception as e:
                    print(f"‚ùå –û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è {archive['name']}: {e}")
        
        return cleaned

if __name__ == "__main__":
    # –¢–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—É—Å–∫
    print("üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ AI-–∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞...")
    
    analyzer = AIAnalyzer()
    manager = ArchiveManager()
    
    archives = manager.list_archives()
    print(f"üìÅ –ù–∞–π–¥–µ–Ω–æ –∞—Ä—Ö–∏–≤–æ–≤: {len(archives)}")
    
    if archives:
        print("üîç –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é –ø–µ—Ä–≤—ã–π –∞—Ä—Ö–∏–≤...")
        result = analyzer.analyze_telegram_archive(archives[0]['path'])
        print("\n" + result['summary'])
    else:
        print("üì≠ –ê—Ä—Ö–∏–≤—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã. –°–Ω–∞—á–∞–ª–∞ –æ—Ç–ø—Ä–∞–≤—å—Ç–µ –∞—Ä—Ö–∏–≤—ã —Å –ü–ö2.")
    
    # –û—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö –∞—Ä—Ö–∏–≤–æ–≤
    cleaned = manager.cleanup_old_archives(days_old=7)
    print(f"üßπ –û—á–∏—â–µ–Ω–æ —Å—Ç–∞—Ä—ã—Ö –∞—Ä—Ö–∏–≤–æ–≤: {cleaned}")